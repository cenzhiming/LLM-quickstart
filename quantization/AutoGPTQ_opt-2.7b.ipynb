{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_D9kG_efts3"
      },
      "source": [
        "# Transformers 模型量化技术：GPTQ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7rrVrEFMGiu"
      },
      "source": [
        "![image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/159_autogptq_transformers/thumbnail.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wwsg6nCwoThm"
      },
      "source": [
        "2022年，Frantar等人发表了论文 [GPTQ：Accurate Post-Training Quantization for Generative Pre-trained Transformers](https://arxiv.org/abs/2210.17323)。\n",
        "\n",
        "这篇论文详细介绍了一种训练后量化算法，适用于所有通用的预训练 Transformer模型，同时只有微小的性能下降。\n",
        "\n",
        "GPTQ算法需要通过对量化模型进行推理来校准模型的量化权重。详细的量化算法在[原始论文](https://arxiv.org/pdf/2210.17323.pdf)中有描述。\n",
        "\n",
        "基于[`auto-gptq`](https://github.com/PanQiWei/AutoGPTQ.git) 开源实现库，transformers 支持使用GPTQ算法量化的模型。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2ZTO3OSoL5T"
      },
      "source": [
        "## 使用 GPTQ 量化模型\n",
        "\n",
        "为了使用 `auto-gptq` 库量化一个模型，我们需要向量化器传递一个数据集。\n",
        "\n",
        "通常有两种方式构造数据集：\n",
        "- 量化器支持的默认数据集（包括`['wikitext2','c4','c4-new','ptb','ptb-new']`）\n",
        "- 一个字符串列表（这些字符串将被用作数据集）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_DJf_pA-lnq"
      },
      "source": [
        "### 使用 GPTQ 算法支持的默认数据集来量化"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-H2019RkoiM-"
      },
      "source": [
        "在下面的示例中，让我们尝试使用`\"wikitext2\"`数据集将模型量化为4位精度。支持的精度有`[2, 4, 6, 8]`。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gEdeadOEoRzq"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig\n",
        "import torch\n",
        "\n",
        "model_name_or_path = \"facebook/opt-2.7b\"\n",
        "\n",
        "quantization_config = GPTQConfig(\n",
        "     bits=4, # 量化精度\n",
        "     group_size=128,\n",
        "     dataset=\"wikitext2\",\n",
        "     desc_act=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rPA0zFSP_ql"
      },
      "source": [
        "#### 逐层量化\n",
        "\n",
        "关于 `CUDA extension not installed` 的说明: https://github.com/AutoGPTQ/AutoGPTQ/issues/249"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optimum\n",
        "!pip install bitsandbytes\n",
        "!pip install optimum"
      ],
      "metadata": {
        "id": "ZH4Hsd9qQcQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "import torch\n",
        "import transformers\n",
        "import bitsandbytes\n",
        "import accelerate\n",
        "\n",
        "print(\"--- System and Library Versions ---\")\n",
        "print(f\"Python version: {sys.version}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Transformers version: {transformers.__version__}\")\n",
        "print(f\"BitsAndBytes version: {bitsandbytes.__version__}\")\n",
        "print(f\"Accelerate version: {accelerate.__version__}\")\n",
        "\n",
        "print(\"\\n--- CUDA Availability ---\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"PyTorch CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
        "    print(f\"Current GPU name: {torch.cuda.get_device_name(0)}\")\n",
        "    # Check NVIDIA driver version (requires nvidia-smi to be in PATH)\n",
        "    try:\n",
        "        import subprocess\n",
        "        driver_output = subprocess.check_output(\"nvidia-smi --query-gpu=driver_version --format=csv,noheader\", shell=True).decode().strip()\n",
        "        print(f\"NVIDIA Driver Version: {driver_output}\")\n",
        "    except Exception:\n",
        "        print(\"Could not retrieve NVIDIA driver version (nvidia-smi not found or error).\")\n",
        "else:\n",
        "    print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
        "    print(\"!!! WARNING: CUDA IS NOT AVAILABLE. Quantization WILL NOT WORK. !!!\")\n",
        "    print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
        "\n",
        "print(\"\\n--- BitsAndBytes CUDA Binding Check ---\")\n",
        "try:\n",
        "    from bitsandbytes.cuda_setup.main import get_compute_capability\n",
        "    compute_cap = get_compute_capability()\n",
        "    print(f\"BitsAndBytes CUDA Compute Capability: {compute_cap}\")\n",
        "    print(\"BitsAndBytes seems to have found its CUDA binding successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"ERROR: Could not get compute capability from bitsandbytes: {e}\")\n",
        "    print(\"This indicates a severe issue with bitsandbytes' CUDA binding or installation.\")\n",
        "    print(\"!!! Quantization WILL NOT WORK until this is resolved. !!!\")\n",
        "\n",
        "print(\"\\n--- Environment Variables (relevant ones) ---\")\n",
        "print(f\"LD_LIBRARY_PATH (Linux/macOS): {os.environ.get('LD_LIBRARY_PATH', 'Not set')}\")\n",
        "print(f\"PATH (partial, first 5 entries): {os.environ.get('PATH', 'Not set').split(os.pathsep)[:5]}...\")"
      ],
      "metadata": {
        "id": "IGOv8VCKRuD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade optimum"
      ],
      "metadata": {
        "id": "KK8s71WUT92Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tv04by6mP_qm"
      },
      "outputs": [],
      "source": [
        "quant_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name_or_path,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map='auto')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYbH6BUfP_qm"
      },
      "source": [
        "### 实测 GPTQ 量化模型：GPU显存占用峰值超过10GB\n",
        "\n",
        "```shell\n",
        "Wed Feb  7 22:03:42 2024\n",
        "+---------------------------------------------------------------------------------------+\n",
        "| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |\n",
        "|-----------------------------------------+----------------------+----------------------+\n",
        "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
        "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
        "|                                         |                      |               MIG M. |\n",
        "|=========================================+======================+======================|\n",
        "|   0  Tesla T4                       Off | 00000000:00:0D.0 Off |                    0 |\n",
        "| N/A   66C    P0              69W /  70W |   9719MiB / 15360MiB |    100%      Default |\n",
        "|                                         |                      |                  N/A |\n",
        "+-----------------------------------------+----------------------+----------------------+\n",
        "\n",
        "+---------------------------------------------------------------------------------------+\n",
        "| Processes:                                                                            |\n",
        "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
        "|        ID   ID                                                             Usage      |\n",
        "|=======================================================================================|\n",
        "|    0   N/A  N/A      2334      C   /root/miniconda3/bin/python               10714MiB |\n",
        "+---------------------------------------------------------------------------------------+\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NP3sd8X-QEo"
      },
      "source": [
        "### 检查量化模型正确性\n",
        "\n",
        "通过检查`线性层的属性`来确保模型已正确量化，它们应该包含`qweight`和`qzeros`属性，这些属性应该是`torch.int32`数据类型。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJRv3bGyZtcB"
      },
      "outputs": [],
      "source": [
        "quant_model.model.decoder.layers[0].self_attn.q_proj.__dict__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zIF80BdpP_qm"
      },
      "outputs": [],
      "source": [
        "# 保存模型权重\n",
        "quant_model.save_pretrained(\"models/opt-2.7b-gptq\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ve_FkzIy-gZ5"
      },
      "source": [
        "#### 使用 GPU 加载模型并生成文本"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoXV8zrIuORr",
        "outputId": "44ccab95-8a4e-4d5e-dc25-a90b7bdb5f4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Merry Christmas! I'm glad to see you're still here.\n",
            "Thank you! I'm glad to be here too.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
        "\n",
        "text = \"Merry Christmas! I'm glad to\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(0)\n",
        "\n",
        "out = quant_model.generate(**inputs, max_new_tokens=64)\n",
        "print(tokenizer.decode(out[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhCaFfJ5-k1l"
      },
      "source": [
        "### 使用自定义数据集量化模型（灵活可扩展，前提是准备好数据）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dN-hRO_vtJTB"
      },
      "source": [
        "下面演示通过传递自定义数据集来量化一个模型。\n",
        "\n",
        "通过字符串列表来自定义一个数据集，建议样本数不少于128（样本数太少会影响模型性能）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cmll3BZntI88"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, GPTQConfig, AutoTokenizer\n",
        "\n",
        "model_name_or_path = \"facebook/opt-2.7b\"\n",
        "custom_dataset = [\"auto-gptq is an easy-to-use model quantization library with user-friendly apis, based on GPTQ algorithm.\"]\n",
        "\n",
        "custom_quantization_config = GPTQConfig(\n",
        "    bits=4,\n",
        "    group_size=128,\n",
        "    desc_act=False,\n",
        "    dataset=custom_dataset\n",
        ")\n",
        "\n",
        "custom_quant_model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
        "                                                          quantization_config=custom_quantization_config,\n",
        "                                                          torch_dtype=torch.float16,\n",
        "                                                          device_map=\"auto\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAWDL4Vr-8T2"
      },
      "source": [
        "相比使用默认数据集，未经精心准备的自定义数据集会明显降低模型性能"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6yeN0zcxUGg"
      },
      "outputs": [],
      "source": [
        "text = \"Merry Christmas! I'm glad to\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(0)\n",
        "\n",
        "out = custom_quant_model.generate(**inputs, max_new_tokens=64)\n",
        "print(tokenizer.decode(out[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7m9WTQRP_qn"
      },
      "source": [
        "## [Optional]Homework: 使用 GPTQ 算法量化 OPT-6.7B 模型\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JA4jzzTP_qn"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}